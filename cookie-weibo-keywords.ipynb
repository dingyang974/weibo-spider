{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5773322",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d7615e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请手动登录微博并获取目标页面的Cookie：\n",
      "1. 打开Chrome浏览器，登录你的微博账号。\n",
      "2. 访问微博高级搜索页面：https://s.weibo.com/\n",
      "3. 按下F12打开开发者工具，切换到Network面板。\n",
      "4. 刷新页面，在左侧列表中找到任意一个请求，点击它，在右侧的Headers选项卡中找到Request Headers下的cookie，复制其值。\n",
      "请粘贴获取到的Cookie：XSRF-TOKEN=zPIJr7U2rjYtkWe22U6_rnpY; SCF=Ak6ph5Fk1uPRSLkJscl_C8llTMJ-SErIRuV8P9JdlNr3-uZsOj5AZD3qBHgWjRSO6QRgDzgIkS9S3ZciDa3Ll0I.; SUB=_2A25FZIU3DeRhGeFL71oS9C3JyziIHXVmG5j_rDV8PUNbmtAbLXfskW9NQhGYLoCSuFlCfX0E3ldt_7NtfT2-kuaw; SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9WWJQUYTWDrUcICTQl2cLgxA5JpX5KzhUgL.FoMfShn0ShefehB2dJLoIpQLxK-L122LB-2LxK-L122LB-8VUg4fdJ8VqBtt; ALF=02_1753776743; _s_tentry=weibo.com; Apache=800341091193.9557.1751184867191; SINAGLOBAL=800341091193.9557.1751184867191; ULV=1751184867266:1:1:1:800341091193.9557.1751184867191:; WBPSESS=lUUKvpExzH8yjl_-2iqM6BAVpX9Ppx5rm3aOWmOXARdEtIH-5wsHU5Cslu7VakQr4r491LITBMCFwhS5gVw6zobjZ34AuU0e035kidvDh8GOaVeTLf1bdly4dMKlIuLx36syoDjhFLI0PMKsw1NKGA==\n",
      "请输入搜索关键词: 黑龙江 北黑线 列车 脱轨\n",
      "请输入最大搜索页数 (默认50): 2\n",
      "正在搜索第 1 页...\n",
      "找到文章: 头条新闻 - 【“11·1”#黑河列车脱轨20名相关责任人被处理#】据央视... (评论: 0, 转发: 0, 发布时间: 未知时间)\n",
      "找到文章: 三联生活周刊 - 【官方公布#黑龙江一旅客列车行驶中脱轨事故调查报告#】据国家... (评论: 0, 转发: 0, 发布时间: 未知时间)\n",
      "找到文章: 凤凰周刊 - 【官方公布#黑龙江一旅客列车行驶中脱轨事故调查报告#】据国家... (评论: 0, 转发: 0, 发布时间: 未知时间)\n",
      "找到文章: 财新网 - 【“11·1”北黑线脱轨事故公布原因 黑河铁路一二把手均免职... (评论: 0, 转发: 0, 发布时间: 未知时间)\n",
      "找到文章: 财新网 - 【能源内参｜“11·1”北黑线脱轨事故公布原因 黑河铁路一二... (评论: 0, 转发: 0, 发布时间: 未知时间)\n",
      "找到文章: 中国新闻网 - 【#黑龙江一旅客列车运行中脱轨# 无人员伤亡】#K5133次... (评论: 0, 转发: 0, 发布时间: 未知时间)\n",
      "找到文章: 新浪热点 - 【#黑龙江一旅客列车运输中脱轨#】记者从黑河铁路集团获悉，2... (评论: 0, 转发: 0, 发布时间: 未知时间)\n",
      "找到文章: 环球时报 - 【#黑龙江一旅客列车运行中脱轨# 无人员伤亡】#K5133次... (评论: 0, 转发: 0, 发布时间: 未知时间)\n",
      "找到文章: 每日经济新闻 - #K5133次列车脱轨事故原因公布#【突发！K5133次列车... (评论: 0, 转发: 0, 发布时间: 未知时间)\n",
      "第 1 页找到 9 篇文章，已获取 9 篇文章\n",
      "正在搜索第 2 页...\n",
      "找到文章: 航空物语 - 铁路部门通报：2023年10月15日3时50分许，哈尔滨开往... (评论: 0, 转发: 0, 发布时间: 未知时间)\n",
      "找到文章: 凤凰网 - #K5133次旅客列车脱轨#【#黑龙江北黑线一旅客列车运输中... (评论: 0, 转发: 0, 发布时间: 未知时间)\n",
      "找到文章: 新京报我们视频 - 【#北黑线列车脱轨较大事故调查结果公布#：制动系统失效工程车... (评论: 0, 转发: 0, 发布时间: 未知时间)\n",
      "找到文章: 大河报 - 【#黑龙江一列车脱轨无人员伤亡#，原因正在调查中】10月15... (评论: 0, 转发: 0, 发布时间: 未知时间)\n",
      "找到文章: 潇湘晨报 - 【黑龙江北黑线#K5133次旅客列车运行中脱轨#，无人员伤亡... (评论: 0, 转发: 0, 发布时间: 未知时间)\n",
      "找到文章: 南方周末 - 【免职、撤职、开除！#官方公布K5133列车脱轨事故处理结果... (评论: 0, 转发: 0, 发布时间: 未知时间)\n",
      "找到文章: 西充交警 - 【#黑龙江一旅客列车运行中脱轨# 无人员伤亡】#K5133次... (评论: 0, 转发: 0, 发布时间: 未知时间)\n",
      "找到文章: 东海发布 - 【#黑龙江一旅客列车运行中脱轨# 无人员伤亡】#K5133次... (评论: 0, 转发: 0, 发布时间: 未知时间)\n",
      "第 2 页找到 8 篇文章，已获取 17 篇文章\n",
      "共找到 17 篇热门文章\n",
      "数据已保存到 微博热门文章.csv\n",
      "\n",
      "处理文章 1/17: 【“11·1”#黑河列车脱轨20名相关责任人被处理#】据央视新闻报道：1月3日，国家铁路局公布“11...\n",
      "评论数为0，跳过 (实际评论数: 0)\n",
      "\n",
      "处理文章 2/17: 【官方公布#黑龙江一旅客列车行驶中脱轨事故调查报告#】据国家铁路局网站1月3日消息，沈阳铁路监督管理...\n",
      "评论数为0，跳过 (实际评论数: 0)\n",
      "\n",
      "处理文章 3/17: 【官方公布#黑龙江一旅客列车行驶中脱轨事故调查报告#】据国家铁路局网站1月3日消息，沈阳铁路监督管理...\n",
      "评论数为0，跳过 (实际评论数: 0)\n",
      "\n",
      "处理文章 4/17: 【“11·1”北黑线脱轨事故公布原因 黑河铁路一二把手均免职】近日，沈阳铁路监督管理局通报“11·1...\n",
      "评论数为0，跳过 (实际评论数: 0)\n",
      "\n",
      "处理文章 5/17: 【能源内参｜“11·1”北黑线脱轨事故公布原因 黑河铁路一二把手均免职；美总统拜登否决日本制铁收购美...\n",
      "评论数为0，跳过 (实际评论数: 0)\n",
      "\n",
      "处理文章 6/17: 【#黑龙江一旅客列车运行中脱轨# 无人员伤亡】#K5133次旅客列车运行中脱轨# 记者从黑河铁路集团...\n",
      "评论数为0，跳过 (实际评论数: 0)\n",
      "\n",
      "处理文章 7/17: 【#黑龙江一旅客列车运输中脱轨#】记者从黑河铁路集团获悉，2023年10月15日3时50分许，哈尔滨...\n",
      "评论数为0，跳过 (实际评论数: 0)\n",
      "\n",
      "处理文章 8/17: 【#黑龙江一旅客列车运行中脱轨# 无人员伤亡】#K5133次旅客列车运行中脱轨# 记者从黑河铁路集团...\n",
      "评论数为0，跳过 (实际评论数: 0)\n",
      "\n",
      "处理文章 9/17: #K5133次列车脱轨事故原因公布#【突发！K5133次列车凌晨脱轨，事故原因公布】据央视新闻报道，...\n",
      "评论数为0，跳过 (实际评论数: 0)\n",
      "\n",
      "处理文章 10/17: 铁路部门通报：2023年10月15日3时50分许，哈尔滨开往黑河的K5133次旅客列车运行到孙吴至黑...\n",
      "评论数为0，跳过 (实际评论数: 0)\n",
      "\n",
      "处理文章 11/17: #K5133次旅客列车脱轨#【#黑龙江北黑线一旅客列车运输中脱轨#】记者从黑河铁路集团获悉，2023...\n",
      "评论数为0，跳过 (实际评论数: 0)\n",
      "\n",
      "处理文章 12/17: 【#北黑线列车脱轨较大事故调查结果公布#：制动系统失效工程车无人值守发生溜逸 与旅客列车相撞造成脱轨...\n",
      "评论数为0，跳过 (实际评论数: 0)\n",
      "\n",
      "处理文章 13/17: 【#黑龙江一列车脱轨无人员伤亡#，原因正在调查中】10月15日，黑龙江黑河。据央视新闻消息，从黑河铁...\n",
      "评论数为0，跳过 (实际评论数: 0)\n",
      "\n",
      "处理文章 14/17: 【黑龙江北黑线#K5133次旅客列车运行中脱轨#，无人员伤亡，预计今日可恢复通行】据央视新闻消息，记...\n",
      "评论数为0，跳过 (实际评论数: 0)\n",
      "\n",
      "处理文章 15/17: 【免职、撤职、开除！#官方公布K5133列车脱轨事故处理结果#】据央视新闻报道，10月15日3时50...\n",
      "评论数为0，跳过 (实际评论数: 0)\n",
      "\n",
      "处理文章 16/17: 【#黑龙江一旅客列车运行中脱轨# 无人员伤亡】#K5133次旅客列车运行中脱轨# 记者从黑河铁路集团...\n",
      "评论数为0，跳过 (实际评论数: 0)\n",
      "\n",
      "处理文章 17/17: 【#黑龙江一旅客列车运行中脱轨# 无人员伤亡】#K5133次旅客列车运行中脱轨# 记者从黑河铁路集团...\n",
      "评论数为0，跳过 (实际评论数: 0)\n",
      "\n",
      "所有处理完成! 共获取 17 篇热门文章，0 条评论\n",
      "热门文章数据已保存到: 微博热门文章.csv\n",
      "热门评论数据已保存到: 微博热门评论.csv\n",
      "程序运行总时长: 0.68 分钟\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import csv\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import quote\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "def get_cookie_manually():\n",
    "    print(\"请手动登录微博并获取目标页面的Cookie：\")\n",
    "    print(\"1. 打开Chrome浏览器，登录你的微博账号。\")\n",
    "    print(\"2. 访问微博高级搜索页面：https://s.weibo.com/\")\n",
    "    print(\"3. 按下F12打开开发者工具，切换到Network面板。\")\n",
    "    print(\"4. 刷新页面，在左侧列表中找到任意一个请求，点击它，在右侧的Headers选项卡中找到Request Headers下的cookie，复制其值。\")\n",
    "    cookie = input(\"请粘贴获取到的Cookie：\")\n",
    "    return cookie\n",
    "\n",
    "def get_weibo_session(cookie):\n",
    "    \"\"\"创建带有重试机制的会话\"\"\"\n",
    "    session = requests.Session()\n",
    "    retry_strategy = Retry(\n",
    "        total=5,\n",
    "        backoff_factor=1,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"GET\"]\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    \n",
    "    headers = {\n",
    "        'cookie': cookie,\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36',\n",
    "        'referer': 'https://s.weibo.com/'\n",
    "    }\n",
    "    session.headers.update(headers)\n",
    "    return session\n",
    "\n",
    "def search_weibo(session, keyword, max_pages=50):\n",
    "    \"\"\"搜索微博文章 - 只筛选热门帖子\"\"\"\n",
    "    base_url = \"https://s.weibo.com/weibo\"\n",
    "    \n",
    "    params = {\n",
    "        'q': keyword,\n",
    "        'xsort': 'hot',  # 按热门排序\n",
    "        'suball': 1,     # 包含所有子类型\n",
    "        'Refer': 'g',\n",
    "        'page': 1\n",
    "    }\n",
    "    \n",
    "    articles = []\n",
    "    page_count = 0\n",
    "    \n",
    "    while page_count < max_pages:\n",
    "        page_count += 1\n",
    "        print(f\"正在搜索第 {page_count} 页...\")\n",
    "        params['page'] = page_count\n",
    "        \n",
    "        try:\n",
    "            response = session.get(base_url, params=params, timeout=30)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"搜索请求失败，状态码: {response.status_code}\")\n",
    "                print(f\"响应内容: {response.text[:200]}...\")\n",
    "                break\n",
    "                \n",
    "            # 检查是否被重定向到验证页面\n",
    "            if \"安全验证\" in response.text or \"验证码\" in response.text:\n",
    "                print(\"⚠️ 遇到安全验证，请手动解决验证问题后再继续\")\n",
    "                input(\"按回车键继续...\")\n",
    "                continue\n",
    "                \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            cards = soup.find_all('div', class_='card-wrap')\n",
    "            \n",
    "            if not cards:\n",
    "                print(\"没有找到更多文章，停止搜索\")\n",
    "                break\n",
    "                \n",
    "            for card in cards:\n",
    "                # 过滤广告和推荐内容\n",
    "                if card.find('div', class_='card-top'):\n",
    "                    continue\n",
    "                    \n",
    "                # 获取文章信息\n",
    "                mid = card.get('mid', '')\n",
    "                user_info = card.find('a', class_='name')\n",
    "                user_name = user_info.text.strip() if user_info else \"未知用户\"\n",
    "                \n",
    "                # 处理用户链接\n",
    "                user_link = user_info['href'] if user_info and user_info.get('href') else \"\"\n",
    "                user_id = \"\"\n",
    "                if user_link:\n",
    "                    # 提取用户ID - 更健壮的方法\n",
    "                    user_id_match = re.search(r'/(\\d+)(\\?|\\b)', user_link)\n",
    "                    if user_id_match:\n",
    "                        user_id = user_id_match.group(1)\n",
    "                \n",
    "                # 获取内容\n",
    "                content = card.find('p', class_='txt')\n",
    "                content_text = content.text.strip() if content else \"\"\n",
    "                \n",
    "                # 获取发布时间 - 改进方法\n",
    "                post_time = \"未知时间\"\n",
    "                time_tag = card.find('p', class_='from')\n",
    "                if time_tag:\n",
    "                    time_a = time_tag.find('a')\n",
    "                    if time_a:\n",
    "                        post_time = time_a.text.strip()\n",
    "                \n",
    "                # 获取文章链接 - 改进方法\n",
    "                article_link = \"\"\n",
    "                # 从时间标签获取链接\n",
    "                if time_tag and time_tag.find('a'):\n",
    "                    link_tag = time_tag.find('a')\n",
    "                    if link_tag and 'href' in link_tag.attrs:\n",
    "                        article_link = \"https://s.weibo.com\" + link_tag['href'] if not link_tag['href'].startswith('http') else link_tag['href']\n",
    "                \n",
    "                # 从内容区域获取链接\n",
    "                if not article_link:\n",
    "                    content_link = card.find('a', class_='t')\n",
    "                    if content_link and 'href' in content_link.attrs:\n",
    "                        article_link = \"https://s.weibo.com\" + content_link['href'] if not content_link['href'].startswith('http') else content_link['href']\n",
    "                \n",
    "                # 从其他可能的元素获取链接\n",
    "                if not article_link:\n",
    "                    possible_links = card.find_all('a', href=True)\n",
    "                    for link in possible_links:\n",
    "                        if '/detail/' in link['href'] or '/status/' in link['href']:\n",
    "                            article_link = \"https://s.weibo.com\" + link['href'] if not link['href'].startswith('http') else link['href']\n",
    "                            break\n",
    "                \n",
    "                # 获取评论数和转发数 - 改进方法\n",
    "                comment_count = 0\n",
    "                repost_count = 0\n",
    "                \n",
    "                # 查找互动区域\n",
    "                card_act = card.find('div', class_='card-act')\n",
    "                if card_act:\n",
    "                    # 查找所有li标签\n",
    "                    li_tags = card_act.find_all('li')\n",
    "                    for li in li_tags:\n",
    "                        # 查找a标签\n",
    "                        a_tag = li.find('a')\n",
    "                        if a_tag:\n",
    "                            text = a_tag.text.strip()\n",
    "                            # 提取数字\n",
    "                            num_text = re.sub(r'\\D', '', text)\n",
    "                            if num_text:\n",
    "                                num = int(num_text)\n",
    "                                \n",
    "                                # 根据文本内容判断类型\n",
    "                                if '转发' in text:\n",
    "                                    repost_count = num\n",
    "                                elif '评论' in text:\n",
    "                                    comment_count = num\n",
    "                                elif '赞' in text:\n",
    "                                    pass  # 不需要赞数\n",
    "                \n",
    "                # 提取文章ID - 改进方法\n",
    "                if not mid:\n",
    "                    # 尝试从文章链接中提取\n",
    "                    if article_link:\n",
    "                        # 尝试匹配 /weibo? 格式的链接\n",
    "                        mid_match = re.search(r'/(\\d+)/(\\w+)', article_link)\n",
    "                        if mid_match:\n",
    "                            mid = mid_match.group(2)\n",
    "                        else:\n",
    "                            # 尝试匹配 /detail/ 格式的链接\n",
    "                            mid_match = re.search(r'/detail/(\\w+)', article_link)\n",
    "                            if mid_match:\n",
    "                                mid = mid_match.group(1)\n",
    "                \n",
    "                # 如果还没有提取到mid，生成一个临时ID\n",
    "                if not mid:\n",
    "                    mid = f\"temp_{int(time.time())}\"\n",
    "                \n",
    "                article = {\n",
    "                    'mid': mid,\n",
    "                    'user_id': user_id,\n",
    "                    'user_name': user_name,\n",
    "                    'content': content_text,\n",
    "                    'post_time': post_time,\n",
    "                    'article_link': article_link,\n",
    "                    'comment_count': comment_count,\n",
    "                    'repost_count': repost_count\n",
    "                }\n",
    "                \n",
    "                articles.append(article)\n",
    "                print(f\"找到文章: {user_name} - {content_text[:30]}... (评论: {comment_count}, 转发: {repost_count}, 发布时间: {post_time})\")\n",
    "            \n",
    "            print(f\"第 {page_count} 页找到 {len(cards)} 篇文章，已获取 {len(articles)} 篇文章\")\n",
    "            \n",
    "            # 检查是否有下一页\n",
    "            next_page = soup.find('a', class_='next')\n",
    "            if not next_page:\n",
    "                print(\"已到达最后一页\")\n",
    "                break\n",
    "                \n",
    "            # 避免频繁请求\n",
    "            time.sleep(5 + page_count % 3)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"搜索过程中发生错误: {str(e)}\")\n",
    "            break\n",
    "    \n",
    "    return articles\n",
    "\n",
    "def get_comments(session, article):\n",
    "    \"\"\"获取单篇文章的所有评论\"\"\"\n",
    "    # 如果文章链接缺失，尝试使用其他方式获取评论\n",
    "    if not article.get('article_link') and not article.get('mid'):\n",
    "        print(\"文章链接和ID都缺失，无法获取评论\")\n",
    "        return []\n",
    "    \n",
    "    # 如果缺少用户ID，尝试从文章链接中提取\n",
    "    if not article.get('user_id') and article.get('article_link'):\n",
    "        user_id_match = re.search(r'/(\\d+)/', article['article_link'])\n",
    "        if user_id_match:\n",
    "            article['user_id'] = user_id_match.group(1)\n",
    "    \n",
    "    if not article.get('user_id'):\n",
    "        print(\"⚠️ 无法提取用户ID，使用默认值\")\n",
    "        article['user_id'] = \"0\"\n",
    "    \n",
    "    comments = []\n",
    "    next_param = 'count=20'\n",
    "    page_count = 0\n",
    "    max_retries = 3\n",
    "    retry_count = 0\n",
    "    total_comments = 0\n",
    "    expected_total = article.get('comment_count', 0)\n",
    "    \n",
    "    print(f\"开始获取文章 {article['mid']} 的评论 (预计: {expected_total})...\")\n",
    "    \n",
    "    while True:\n",
    "        page_count += 1\n",
    "        url = f'https://weibo.com/ajax/statuses/buildComments?is_reload=1&id={article[\"mid\"]}&is_show_bulletin=2&is_mix=0&{next_param}&uid={article[\"user_id\"]}&fetch_level=0&locale=zh-CN'\n",
    "        \n",
    "        try:\n",
    "            response = session.get(url, timeout=30)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"评论请求失败，状态码: {response.status_code}\")\n",
    "                if retry_count < max_retries:\n",
    "                    retry_count += 1\n",
    "                    print(f\"第 {retry_count} 次重试...\")\n",
    "                    time.sleep(10)\n",
    "                    continue\n",
    "                else:\n",
    "                    print(\"重试次数已达上限，停止获取\")\n",
    "                    break\n",
    "            \n",
    "            json_data = response.json()\n",
    "            data_list = json_data.get('data', [])\n",
    "            max_id = json_data.get('max_id', 0)\n",
    "            \n",
    "            if not data_list:\n",
    "                print(\"没有评论数据\")\n",
    "                break\n",
    "                \n",
    "            for comment in data_list:\n",
    "                text_raw = comment.get('text_raw', '')\n",
    "                id = comment.get('id', '')\n",
    "                created_at = comment.get('created_at', '')\n",
    "                like_counts = comment.get('like_counts', 0)\n",
    "                total_number = comment.get('total_number', 0)\n",
    "                user_info = comment.get('user', {})\n",
    "                screen_name = user_info.get('screen_name', '')\n",
    "                user_id = user_info.get('id', '')\n",
    "                \n",
    "                comment_data = {\n",
    "                    'article_mid': article['mid'],\n",
    "                    'article_content': article['content'][:100] if article.get('content') else \"\",\n",
    "                    'article_user': article.get('user_name', \"\"),\n",
    "                    'comment_id': id,\n",
    "                    'user_id': user_id,\n",
    "                    'user_name': screen_name,\n",
    "                    'content': text_raw,\n",
    "                    'like_counts': like_counts,\n",
    "                    'created_at': created_at\n",
    "                }\n",
    "                \n",
    "                comments.append(comment_data)\n",
    "                total_comments += 1\n",
    "            \n",
    "            print(f\"已获取 {len(data_list)} 条评论，总计: {total_comments}/{expected_total}\")\n",
    "            \n",
    "            if max_id and max_id != 0:\n",
    "                next_param = f'max_id={max_id}&count=20'\n",
    "            else:\n",
    "                print(\"已到达最后一页评论\")\n",
    "                break\n",
    "                \n",
    "            # 检查是否已获取所有评论\n",
    "            if expected_total > 0 and total_comments >= expected_total:\n",
    "                print(f\"已获取所有 {expected_total} 条评论\")\n",
    "                break\n",
    "                \n",
    "            # 避免频繁请求\n",
    "            time.sleep(3 + page_count % 2)\n",
    "            retry_count = 0  # 重置重试计数器\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"获取评论时发生错误: {str(e)}\")\n",
    "            if retry_count < max_retries:\n",
    "                retry_count += 1\n",
    "                print(f\"第 {retry_count} 次重试...\")\n",
    "                time.sleep(10)\n",
    "            else:\n",
    "                print(\"重试次数已达上限，停止获取\")\n",
    "                break\n",
    "    \n",
    "    return comments\n",
    "\n",
    "def save_to_csv(data, filename, fieldnames):\n",
    "    \"\"\"保存数据到CSV文件\"\"\"\n",
    "    try:\n",
    "        with open(filename, 'a', encoding='utf-8-sig', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "            if f.tell() == 0:  # 如果是新文件，写入表头\n",
    "                writer.writeheader()\n",
    "            writer.writerows(data)\n",
    "        print(f\"数据已保存到 {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"保存文件时出错: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    # 获取Cookie\n",
    "    cookie = get_cookie_manually()\n",
    "    \n",
    "    # 创建会话\n",
    "    session = get_weibo_session(cookie)\n",
    "    \n",
    "    # 用户输入搜索条件\n",
    "    keyword = input(\"请输入搜索关键词: \")\n",
    "    max_pages = int(input(\"请输入最大搜索页数 (默认50): \") or 50)\n",
    "    \n",
    "    # 搜索文章 - 只获取热门帖子\n",
    "    articles = search_weibo(session, keyword, max_pages)\n",
    "    print(f\"共找到 {len(articles)} 篇热门文章\")\n",
    "    \n",
    "    # 保存文章数据\n",
    "    if articles:\n",
    "        save_to_csv(articles, '微博热门文章.csv', ['mid', 'user_id', 'user_name', 'content', 'post_time', 'article_link', 'comment_count', 'repost_count'])\n",
    "    \n",
    "    # 获取评论\n",
    "    all_comments = []\n",
    "    for i, article in enumerate(articles):\n",
    "        print(f\"\\n处理文章 {i+1}/{len(articles)}: {article.get('content', '')[:50]}...\")\n",
    "        \n",
    "        # 跳过评论数为0的文章\n",
    "        if article.get('comment_count', 0) == 0:\n",
    "            print(f\"评论数为0，跳过 (实际评论数: {article.get('comment_count', 0)})\")\n",
    "            continue\n",
    "            \n",
    "        comments = get_comments(session, article)\n",
    "        all_comments.extend(comments)\n",
    "        \n",
    "        # 每处理完一篇文章保存一次评论\n",
    "        if comments:\n",
    "            save_to_csv(comments, '微博热门评论.csv', ['article_mid', 'article_content', 'article_user', 'comment_id', 'user_id', 'user_name', 'content', 'like_counts', 'created_at'])\n",
    "        else:\n",
    "            print(f\"未获取到评论，但文章显示有 {article.get('comment_count', 0)} 条评论\")\n",
    "        \n",
    "        # 避免频繁请求\n",
    "        time.sleep(5)\n",
    "    \n",
    "    print(f\"\\n所有处理完成! 共获取 {len(articles)} 篇热门文章，{len(all_comments)} 条评论\")\n",
    "    print(\"热门文章数据已保存到: 微博热门文章.csv\")\n",
    "    print(\"热门评论数据已保存到: 微博热门评论.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    main()\n",
    "    end_time = time.time()\n",
    "    print(f\"程序运行总时长: {(end_time - start_time)/60:.2f} 分钟\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10023bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spider",
   "language": "python",
   "name": "spider"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
